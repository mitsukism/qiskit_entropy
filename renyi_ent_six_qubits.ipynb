{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca116c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import scipy as sc\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_function(nn.Module):\n",
    "    def __init__(self,dimension,hidden_layer):\n",
    "        super(neural_function, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.lin1 = nn.Linear(self.dimension, self.hidden_layer)\n",
    "        self.lin_end = nn.Linear(self.hidden_layer, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = torch.sigmoid(self.lin1(input.float()))\n",
    "        y = self.lin_end(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_density_matrix(dimension):\n",
    "    # Generate a random complex Hermitian matrix\n",
    "    rand_matrix = np.random.rand(dimension, dimension) + 1j * np.random.rand(dimension, dimension)\n",
    "    hermitian_matrix = (rand_matrix + np.conj(rand_matrix.T)) / 2\n",
    "\n",
    "    # Generate eigenvalues and eigenvectors of the Hermitian matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(hermitian_matrix)\n",
    "\n",
    "    # Ensure eigenvalues are non-negative\n",
    "    eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "    # Construct the density matrix\n",
    "    density_matrix = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), np.linalg.inv(eigenvectors)))\n",
    "\n",
    "    # Normalize the density matrix\n",
    "    density_matrix /= np.trace(density_matrix)\n",
    "\n",
    "    return density_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_diag_density_matrix(n):\n",
    "    eigenvalues = np.random.rand(n) \n",
    "    eigenvalues /= eigenvalues.sum()\n",
    "    \n",
    "    while np.any(eigenvalues <= 0):\n",
    "        eigenvalues = np.random.rand(n)\n",
    "        eigenvalues /= eigenvalues.sum()\n",
    "    \n",
    "    rho_diag = np.diag(eigenvalues)\n",
    "    \n",
    "    return rho_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_density_matrix(n):\n",
    "    A = np.random.rand(n, n)\n",
    "    rho = A @ A.T \n",
    "    rho /= np.trace(rho)\n",
    "    \n",
    "    eigvals, _ = np.linalg.eigh(rho)\n",
    "    while np.any(eigvals < 0):\n",
    "        A = np.random.rand(n, n)\n",
    "        rho = A @ A.T\n",
    "        rho /= np.trace(rho)\n",
    "        eigvals, _ = np.linalg.eigh(rho)\n",
    "        \n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930241df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantum circuit settings\n",
    "num_wires = 6\n",
    "num_layers = 1\n",
    "num_shots = 1\n",
    "num_of_samples = 100\n",
    "\n",
    "# dimension of the system\n",
    "N = 64\n",
    "\n",
    "# initiate the quantum device\n",
    "device = qml.device(\"default.mixed\", wires=num_wires, shots=num_of_samples)\n",
    "\n",
    "sigma = np.eye(N)/N\n",
    "\n",
    "@qml.qnode(device)\n",
    "def measure_rho(param):\n",
    "    qml.QubitDensityMatrix(density_matrix, wires=np.arange(num_wires))\n",
    "    qml.RandomLayers(param, wires=np.arange(num_wires), seed=seed_no)\n",
    "    # measure in the computational basis\n",
    "    result = qml.sample(qml.PauliZ(0)), qml.sample(qml.PauliZ(1)), qml.sample(qml.PauliZ(2)), qml.sample(qml.PauliZ(3)), qml.sample(qml.PauliZ(4)), qml.sample(qml.PauliZ(5))\n",
    "    return result\n",
    "\n",
    "@qml.qnode(device)\n",
    "def measure_sigma(param):\n",
    "    qml.QubitDensityMatrix(sigma, wires=np.arange(num_wires))\n",
    "    qml.RandomLayers(param, wires=np.arange(num_wires), seed=seed_no)\n",
    "    # measure in the computational basis\n",
    "    result = qml.sample(qml.PauliZ(0)), qml.sample(qml.PauliZ(1)), qml.sample(qml.PauliZ(2)), qml.sample(qml.PauliZ(3)), qml.sample(qml.PauliZ(4)), qml.sample(qml.PauliZ(5))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a73f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "rho_test = measure_rho(param_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.05\n",
    "\n",
    "# ignore the division by zero\n",
    "np.seterr(divide = 'ignore')\n",
    "\n",
    "sigma_a = np.array(sc.linalg.fractional_matrix_power(sigma, (1.0 - alpha) / (2 * alpha)))\n",
    "\n",
    "Q = np.real(np.trace(sc.linalg.fractional_matrix_power(sigma_a @ rho_test @ sigma_a, alpha)))\n",
    "\n",
    "np.seterr(divide = 'warn')\n",
    "\n",
    "# print the Renyi Entropy\n",
    "print(np.log(N) - ((1.0 / (alpha - 1)) * np.log(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent with neural network\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 4000\n",
    "learning_rate = 0.9\n",
    "num_of_samples = 100\n",
    "dimension = num_wires\n",
    "hidden_layer = 100\n",
    "alpha = 1.05\n",
    "seed_no = 485\n",
    "\n",
    "# initialize the neural network and quantum circuit parameters\n",
    "neural_fn = neural_function(dimension, hidden_layer)\n",
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # parameter-shift for the first term\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(num_wires)])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                forward_sum_1 += np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy())\n",
    "\n",
    "      # normalize this sum\n",
    "            forward_sum_1 = forward_sum_1/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # parameter-shift for the second term of both the terms of the objective function\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(num_wires)])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                backward_sum_1 += np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy())\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_1 = backward_sum_1/num_of_samples\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] = 0.5*alpha*(forward_sum_1 - backward_sum_1)\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init += learning_rate*gradients\n",
    "\n",
    "  # evaluate the gradient with respect to the neural network parameters\n",
    "\n",
    "    # evaluate the first term\n",
    "    grad_w1 = torch.zeros_like(neural_fn.lin1.weight)\n",
    "    grad_b1 = torch.zeros_like(neural_fn.lin1.bias)\n",
    "    grad_w2 = torch.zeros_like(neural_fn.lin_end.weight)\n",
    "    grad_b2 = torch.zeros_like(neural_fn.lin_end.bias)\n",
    "\n",
    "    result = measure_rho(prev_param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(num_wires)])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        nn_result.backward()\n",
    "        grad_w1 += (np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy()))*neural_fn.lin1.weight.grad*(1/num_of_samples)\n",
    "        grad_b1 += (np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy()))*neural_fn.lin1.bias.grad*(1/num_of_samples)\n",
    "        grad_w2 += (np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy()))*neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2 += (np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy()))*neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        neural_fn.lin1.weight.grad.zero_()\n",
    "        neural_fn.lin1.bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "\n",
    "  # evaluate the second term\n",
    "    grad_w1_2 = torch.zeros_like(neural_fn.lin1.weight.grad)\n",
    "    grad_b1_2 = torch.zeros_like(neural_fn.lin1.bias.grad)\n",
    "    grad_w2_2 = torch.zeros_like(neural_fn.lin_end.weight.grad)\n",
    "    grad_b2_2 = torch.zeros_like(neural_fn.lin_end.bias.grad)\n",
    "\n",
    "    # result = measure_sigma(prev_param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        result = np.random.choice([-1, 1], size=num_wires)\n",
    "        nn_result = neural_fn(torch.from_numpy(result.flatten()))\n",
    "        nn_result.backward()\n",
    "        grad_w1_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin1.weight.grad*(1/num_of_samples)\n",
    "        grad_b1_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin1.bias.grad*(1/num_of_samples)\n",
    "        grad_w2_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        neural_fn.lin1.weight.grad.zero_()\n",
    "        neural_fn.lin1.bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "  # evaluate the difference, i.e., the gradient\n",
    "    nn_grad_W1 = grad_w1 - grad_w1_2\n",
    "    nn_grad_b1 = grad_b1 - grad_b1_2\n",
    "    nn_grad_W2 = grad_w2 - grad_w2_2\n",
    "    nn_grad_b2 = grad_b2 - grad_b2_2\n",
    "\n",
    "  # update the NN weights and normalize them\n",
    "    with torch.no_grad():\n",
    "        neural_fn.lin1.weight += learning_rate*(alpha-1)*nn_grad_W1\n",
    "        neural_fn.lin1.bias += learning_rate*(alpha-1)*nn_grad_b1\n",
    "        neural_fn.lin_end.weight += learning_rate*(alpha-1)*nn_grad_W2\n",
    "        neural_fn.lin_end.bias += learning_rate*(alpha-1)*nn_grad_b2\n",
    "\n",
    "  # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(num_wires)])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        first_term += np.exp(((alpha-1)/alpha)*nn_result[0].detach().numpy())\n",
    "\n",
    "  # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "  # # Second term evaluation\n",
    "    second_term = 0\n",
    "    for sample in range(num_of_samples):\n",
    "        result = np.random.choice([-1, 1], size=num_wires)\n",
    "        nn_result = neural_fn(torch.from_numpy(result.flatten()))\n",
    "        second_term += np.exp(nn_result[0].detach().numpy())\n",
    "\n",
    "  # normalize the second term sum\n",
    "    second_term = second_term/num_of_samples\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(np.log(N) -(1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term))\n",
    "\n",
    "  # print the cost\n",
    "    print(np.log(N) - ((1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term)))\n",
    "    \n",
    "#     if (np.isnan(np.log(N) - ((1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term)))):\n",
    "#         with torch.no_grad():\n",
    "#             neural_fn.lin1.weight -= learning_rate*(alpha-1)*nn_grad_W1\n",
    "#             neural_fn.lin1.bias -= learning_rate*(alpha-1)*nn_grad_b1\n",
    "#             neural_fn.lin_end.weight -= learning_rate*(alpha-1)*nn_grad_W2\n",
    "#             neural_fn.lin_end.bias -= learning_rate*(alpha-1)*nn_grad_b2\n",
    "#         param_init = prev_param_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class neural_function(nn.Module):\n",
    "    def __init__(self, dimension, hidden_layers):\n",
    "        super(neural_function, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden_layer_modules = nn.ModuleList()\n",
    "        self.hidden_layer_modules.append(nn.Linear(dimension, hidden_layers[0]))  \n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.hidden_layer_modules.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "\n",
    "        # Output layer \n",
    "        self.lin_end = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = input.float()\n",
    "        \n",
    "        for layer in self.hidden_layer_modules:\n",
    "            y = F.relu(layer(y))  \n",
    "\n",
    "        y = self.lin_end(y)\n",
    "        \n",
    "        return F.softplus(y) # Apply softplus to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent with neural network (softplus version)\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 4000\n",
    "learning_rate = 1.5\n",
    "num_of_samples = 100\n",
    "dimension = num_wires\n",
    "hidden_layer = [100]\n",
    "alpha = 1.05\n",
    "# seed_no = 458\n",
    "\n",
    "\n",
    "# initialize the neural network and quantum circuit parameters\n",
    "neural_fn = neural_function(dimension, hidden_layer)\n",
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # parameter-shift for the first term\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                forward_sum_1 += (nn_result[0].detach().numpy())**((alpha-1)/alpha)\n",
    "\n",
    "      # normalize this sum\n",
    "            forward_sum_1 = forward_sum_1/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # parameter-shift for the second term of both the terms of the objective function\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                backward_sum_1 += (nn_result[0].detach().numpy())**((alpha-1)/alpha)\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_1 = backward_sum_1/num_of_samples\n",
    "\n",
    "      # parameter-shift for the second term\n",
    "            shifted = param_init.copy()\n",
    "    \n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_2 = 0\n",
    "            result = measure_sigma(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                forward_sum_2 += nn_result[0].detach().numpy()\n",
    "\n",
    "      # normalize this sum\n",
    "            forward_sum_2 = forward_sum_2/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # parameter-shift for the second term of both the terms of the objective function\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_2 = 0\n",
    "            result = measure_sigma(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                backward_sum_2 += nn_result[0].detach().numpy()\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_2 = backward_sum_2/num_of_samples\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] = 0.5*alpha * (forward_sum_1 - backward_sum_1) + (0.5*(1-alpha) * (forward_sum_2 - backward_sum_2))\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init += learning_rate*gradients\n",
    "\n",
    "  # evaluate the gradient with respect to the neural network parameters\n",
    "\n",
    "    # evaluate the first term\n",
    "    grad_w1 = []\n",
    "    grad_b1 = []\n",
    "    for layer_index in range(len(hidden_layer)):\n",
    "        grad_w1.append(torch.zeros_like(neural_fn.hidden_layer_modules[layer_index].weight))\n",
    "        grad_b1.append(torch.zeros_like(neural_fn.hidden_layer_modules[layer_index].bias))\n",
    "    grad_w2 = torch.zeros_like(neural_fn.lin_end.weight)\n",
    "    grad_b2 = torch.zeros_like(neural_fn.lin_end.bias)\n",
    "\n",
    "    result = measure_rho(prev_param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        nn_result.backward()\n",
    "        for layer_index in range(len(hidden_layer)):\n",
    "            grad_w1[layer_index] += ((nn_result[0].detach().numpy())**((-1)/alpha))*neural_fn.hidden_layer_modules[layer_index].weight.grad*(1/num_of_samples)\n",
    "            grad_b1[layer_index] += ((nn_result[0].detach().numpy())**((-1)/alpha))*neural_fn.hidden_layer_modules[layer_index].bias.grad*(1/num_of_samples)\n",
    "        grad_w2 += ((nn_result[0].detach().numpy())**((-1)/alpha))*neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2 += ((nn_result[0].detach().numpy())**((-1)/alpha))*neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        for layer_index in range(len(hidden_layer)):\n",
    "            neural_fn.hidden_layer_modules[layer_index].weight.grad.zero_()\n",
    "            neural_fn.hidden_layer_modules[layer_index].bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "\n",
    "  # evaluate the second term\n",
    "    grad_w1_2 = []\n",
    "    grad_b1_2 = []\n",
    "    for layer_index in range(len(hidden_layer)):\n",
    "        grad_w1_2.append(torch.zeros_like(neural_fn.hidden_layer_modules[layer_index].weight))\n",
    "        grad_b1_2.append(torch.zeros_like(neural_fn.hidden_layer_modules[layer_index].bias))\n",
    "    grad_w2_2 = torch.zeros_like(neural_fn.lin_end.weight.grad)\n",
    "    grad_b2_2 = torch.zeros_like(neural_fn.lin_end.bias.grad)\n",
    "\n",
    "    result = measure_sigma(prev_param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        nn_result.backward()\n",
    "        for layer_index in range(len(hidden_layer)):\n",
    "            grad_w1_2[layer_index] += (nn_result[0].detach().numpy()**(1))*neural_fn.hidden_layer_modules[layer_index].weight.grad*(1/num_of_samples)\n",
    "            grad_b1_2[layer_index] += (nn_result[0].detach().numpy()**(1))*neural_fn.hidden_layer_modules[layer_index].bias.grad*(1/num_of_samples)\n",
    "        grad_w2_2 += (nn_result[0].detach().numpy()**(1))*neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2_2 += (nn_result[0].detach().numpy()**(1))*neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        for layer_index in range(len(hidden_layer)):\n",
    "            neural_fn.hidden_layer_modules[layer_index].weight.grad.zero_()\n",
    "            neural_fn.hidden_layer_modules[layer_index].bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "  # evaluate the difference, i.e., the gradient\n",
    "    nn_grad_W1 = []\n",
    "    nn_grad_b1 = []\n",
    "    for layer_index in range(len(hidden_layer)):\n",
    "        nn_grad_W1.append(grad_w1[layer_index] - grad_w1_2[layer_index])\n",
    "        nn_grad_b1.append(grad_b1[layer_index] - grad_b1_2[layer_index])\n",
    "    nn_grad_W2 = grad_w2 - grad_w2_2\n",
    "    nn_grad_b2 = grad_b2 - grad_b2_2\n",
    "\n",
    "  # update the NN weights and normalize them\n",
    "    with torch.no_grad():\n",
    "        for layer_index in range(len(hidden_layer)):\n",
    "            neural_fn.hidden_layer_modules[layer_index].weight += learning_rate*(alpha-1)*nn_grad_W1[layer_index]\n",
    "            neural_fn.hidden_layer_modules[layer_index].bias += learning_rate*(alpha-1)*nn_grad_b1[layer_index]\n",
    "        neural_fn.lin_end.weight += learning_rate*(alpha-1)*nn_grad_W2\n",
    "        neural_fn.lin_end.bias += learning_rate*(alpha-1)*nn_grad_b2\n",
    "\n",
    "  # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        first_term += (nn_result[0].detach().numpy())**((alpha-1)/alpha)\n",
    "\n",
    "  # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "  # # Second term evaluation\n",
    "    second_term = 0\n",
    "    result = measure_sigma(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        second_term += nn_result[0].detach().numpy()\n",
    "\n",
    "  # normalize the second term sum\n",
    "    second_term = second_term/num_of_samples\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(np.log(N) -(1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term))\n",
    "\n",
    "  # print the cost\n",
    "    print(np.log(N) -(1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((-1)**0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43214678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_decimal(arr):\n",
    "    arr = arr.astype(int)\n",
    "    # Replace -1 with 0\n",
    "    arr = np.where(arr == -1, 0, arr)\n",
    "    # Convert the array to a binary string\n",
    "    binary_string = ''.join(str(i) for i in arr)\n",
    "    # Convert the binary string to a decimal value\n",
    "    decimal_value = int(binary_string, 2)\n",
    "    return decimal_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent without neural network.\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 4000\n",
    "learning_rate = 0.98\n",
    "num_of_samples = 100\n",
    "deviation = 1\n",
    "alpha = 1.05\n",
    "seed_no = 42\n",
    "\n",
    "# we store the eigenvalues in a array\n",
    "W = deviation*np.random.rand(64)\n",
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # parameter-shift for the first term\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                forward_sum_1 += np.exp(((alpha-1)/alpha)*W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "      # normalize this sum\n",
    "            forward_sum_1 = forward_sum_1/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # parameter-shift for the second term of both the terms of the objective function\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                backward_sum_1 += np.exp(((alpha-1)/alpha)*W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_1 = backward_sum_1/num_of_samples\n",
    "\n",
    "      # parameter-shift for the second term\n",
    "            shifted = param_init.copy()\n",
    "    \n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_2 = 0\n",
    "            result = measure_sigma(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                forward_sum_2 += np.exp(W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "      # normalize this sum\n",
    "            forward_sum_2 = forward_sum_2/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # parameter-shift for the second term of both the terms of the objective function\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_2 = 0\n",
    "            result = measure_sigma(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "                backward_sum_2 += np.exp(W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_2 = backward_sum_2/num_of_samples\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] = 0.5*alpha * (forward_sum_1 - backward_sum_1) + (0.5*(1-alpha) * (forward_sum_2 - backward_sum_2))\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init += learning_rate*gradients\n",
    "\n",
    "    # evaluate the gradient with respect to the eigenvalues\n",
    "    \n",
    "    dW = np.zeros(64)\n",
    "    result_rho = measure_rho(prev_param_init)\n",
    "    result_sigma = measure_sigma(prev_param_init)\n",
    "    for i in range(64):\n",
    "        E = np.zeros(64)\n",
    "        E[i] = 1\n",
    "        for sample in range(num_of_samples):\n",
    "            sample_result_array_rho = np.array([result_rho[i][sample] for i in range(int(num_wires))])\n",
    "            sample_result_array_sigma = np.array([result_sigma[i][sample] for i in range(int(num_wires))])\n",
    "            dW_first_term = E[binary_array_to_decimal(sample_result_array_rho)]*np.exp(((alpha-1)/alpha)*W[i])\n",
    "            dW_sec_term = E[binary_array_to_decimal(sample_result_array_sigma)]*np.exp(W[i])\n",
    "            dW[i] += dW_first_term - dW_sec_term\n",
    "                        \n",
    "        dW[i] = dW[i]/num_of_samples\n",
    "        \n",
    "    W += learning_rate*(alpha-1)*dW\n",
    "\n",
    "\n",
    "\n",
    "  # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        first_term += np.exp(((alpha-1)/alpha)*W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "  # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "  # # Second term evaluation\n",
    "    second_term = 0\n",
    "    result = measure_sigma(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[i][sample] for i in range(int(num_wires))])\n",
    "        second_term += np.exp(W[binary_array_to_decimal(sample_result_array)])\n",
    "\n",
    "  # normalize the second term sum\n",
    "    second_term = second_term/num_of_samples\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(np.log(N) -(1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term))\n",
    "\n",
    "  # print the cost\n",
    "    print(np.log(N) -(1/(alpha-1))*np.log(alpha*first_term + (1-alpha)*second_term))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
