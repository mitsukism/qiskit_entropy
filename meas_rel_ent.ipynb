{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import scipy as sc\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class neural_function(nn.Module):\n",
    "    def __init__(self, dimension, hidden_layers):\n",
    "        super(neural_function, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Create a list to hold the hidden layer modules\n",
    "        self.hidden_layer_modules = nn.ModuleList()\n",
    "\n",
    "        # Add the input layer\n",
    "        self.hidden_layer_modules.append(nn.Linear(dimension, hidden_layers[0]))\n",
    "\n",
    "        # Add the hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.hidden_layer_modules.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "\n",
    "        # Add the output layer\n",
    "        self.lin_end = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = input.float()\n",
    "\n",
    "        # Forward pass through each hidden layer\n",
    "        for layer in self.hidden_layer_modules:\n",
    "            y = torch.sigmoid(layer(y))\n",
    "        # Forward pass through the output layer\n",
    "        y = self.lin_end(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_diagonal_positive_definite_matrices(dim):\n",
    "    while True:\n",
    "        # Generate random positive values for the diagonal entries\n",
    "        diagonal_values_1 = np.random.rand(dim)\n",
    "        diagonal_values_2 = np.random.rand(dim)\n",
    "\n",
    "        # Construct the diagonal matrices A and B\n",
    "        A = np.diag(diagonal_values_1)\n",
    "        B = np.diag(diagonal_values_2)\n",
    "\n",
    "        # Compute the trace of A\n",
    "        trace_A = np.trace(A)\n",
    "        trace_B = np.trace(B)\n",
    "\n",
    "        # Normalize the matrices to have trace 1\n",
    "        A /= trace_A\n",
    "        B /= trace_B\n",
    "\n",
    "        # Check if matrices A and B commute\n",
    "        commutation_check = np.dot(A, B) - np.dot(B, A)\n",
    "\n",
    "        if np.allclose(commutation_check, np.zeros((dim, dim))):\n",
    "            return A, B\n",
    "\n",
    "# Generate diagonal positive definite matrices A and B with trace 1 and they commute\n",
    "N = 4\n",
    "dim = N # Dimension of the matrices\n",
    "A, B = generate_diagonal_positive_definite_matrices(dim)\n",
    "\n",
    "# Print matrices A and B\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantum circuit settings\n",
    "num_wires = 4\n",
    "num_layers = 10\n",
    "num_shots = 1\n",
    "num_of_samples = 100\n",
    "\n",
    "# initiate the quantum device\n",
    "device = qml.device(\"default.mixed\", wires=num_wires, shots=num_of_samples)\n",
    "\n",
    "@qml.qnode(device)\n",
    "def measure_rho(param):\n",
    "    qml.QubitDensityMatrix(A, wires=[0, 1])\n",
    "    qml.RandomLayers(param, wires=[0, 1])\n",
    "    # measure in the computational basis\n",
    "    result = qml.sample(qml.PauliZ(0)), qml.sample(qml.PauliZ(1))\n",
    "    return result\n",
    "\n",
    "@qml.qnode(device)\n",
    "def measure_sigma(param):\n",
    "    qml.QubitDensityMatrix(sigma, wires=[0, 1])\n",
    "    qml.RandomLayers(param, wires=[0, 1])\n",
    "    # measure in the computational basis\n",
    "    result = qml.sample(qml.PauliZ(0)), qml.sample(qml.PauliZ(1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system size\n",
    "N = 4\n",
    "\n",
    "# ignore the division by zero\n",
    "np.seterr(divide = 'ignore')\n",
    "\n",
    "# quantum relative entropy\n",
    "H_rho = np.real(np.trace(rho_test @ (sc.linalg.logm(rho_test) - sc.linalg.logm(sigma_test))))\n",
    "\n",
    "np.seterr(divide = 'warn')\n",
    "\n",
    "# print the result\n",
    "print(H_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251535c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent (without neural network)\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 300\n",
    "learning_rate = 0.1\n",
    "num_of_samples = 100\n",
    "deviation = 1\n",
    "\n",
    "# initialize the neural network and quantum circuit parameters\n",
    "W = deviation*np.random.rand(3,3)\n",
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # parameter-shift for the first term\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                forward_sum_1 += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the forward sum\n",
    "            forward_sum_1 = forward_sum_1/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_1 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                backward_sum_1 += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_1 = backward_sum_1/num_of_samples\n",
    "\n",
    "      # parameter-shift for the second term\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum_2 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                forward_sum_2 += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the forward sum\n",
    "            forward_sum_2 = forward_sum_2/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum_2 = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                backward_sum_2 += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum_2 = backward_sum_2/num_of_samples\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] =  0.5 * (forward_sum_1 - backward_sum_1) - 0.5 * (forward_sum_2 - backward_sum_2)\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init += learning_rate*gradients\n",
    "\n",
    "  # evaluate the gradient with respect to the eigenvalues\n",
    "\n",
    "  # 1 , 1\n",
    "    E = np.zeros_like(W)\n",
    "    E[1][1] = 1\n",
    "    dW1_first_term = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW1_first_term += E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "\n",
    "    # normalize it\n",
    "    dW1_first_term = dW1_first_term/num_of_samples\n",
    "\n",
    "    dW1_sec_term = 0\n",
    "    result = measure_sigma(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW1_sec_term += E[result[0][sample]][result[1][sample]]*np.exp(W[result[0][sample]][result[1][sample]])\n",
    "\n",
    "    # normalize it\n",
    "    dW1_sec_term = dW1_sec_term/num_of_samples\n",
    "\n",
    "    # 1 , -1\n",
    "    E = np.zeros_like(W)\n",
    "    E[1][-1] = 1\n",
    "    dW2_first_term = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW2_first_term += E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "    # normalize it\n",
    "    dW2_first_term = dW2_first_term/num_of_samples\n",
    "\n",
    "    dW2_sec_term = 0\n",
    "    result = measure_sigma(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW2_sec_term += E[result[0][sample]][result[1][sample]]*np.exp(W[result[0][sample]][result[1][sample]])\n",
    "\n",
    "    # normalize it\n",
    "    dW2_sec_term = dW2_sec_term/num_of_samples\n",
    "\n",
    "    # -1 , 1\n",
    "    E = np.zeros_like(W)\n",
    "    E[-1][1] = 1\n",
    "    dW3_first_term = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW3_first_term += E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "    # normalize it\n",
    "    dW3_first_term = dW3_first_term/num_of_samples\n",
    "\n",
    "    dW3_sec_term = 0\n",
    "    result = measure_sigma(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW3_sec_term += E[result[0][sample]][result[1][sample]]*np.exp(W[result[0][sample]][result[1][sample]])\n",
    "\n",
    "    # normalize it\n",
    "    dW3_sec_term = dW3_sec_term/num_of_samples\n",
    "\n",
    "    # -1 , -1\n",
    "    E = np.zeros_like(W)\n",
    "    E[-1][-1] = 1\n",
    "    dW4_first_term = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW4_first_term += E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "    # normalize it\n",
    "    dW4_first_term = dW4_first_term/num_of_samples\n",
    "\n",
    "    dW4_sec_term = 0\n",
    "    result = measure_sigma(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW4_sec_term += E[result[0][sample]][result[1][sample]]*np.exp(W[result[0][sample]][result[1][sample]])\n",
    "\n",
    "    # normalize it\n",
    "    dW4_sec_term = dW4_sec_term/num_of_samples\n",
    "\n",
    "    # update the eigenvalues\n",
    "    W[1][1] += learning_rate*(dW1_first_term - dW1_sec_term)\n",
    "    W[1][-1] += learning_rate*(dW2_first_term - dW2_sec_term)\n",
    "    W[-1][1] += learning_rate*(dW3_first_term - dW3_sec_term)\n",
    "    W[-1][-1] += learning_rate*(dW4_first_term - dW4_sec_term)\n",
    "\n",
    "    # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        first_term += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "  # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "  # # Second term evaluation\n",
    "    second_term = 0\n",
    "    result = measure_sigma(param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        second_term += np.exp(W[result[0][sample]][result[1][sample]])\n",
    "\n",
    "  # normalize the second term sum\n",
    "    second_term = second_term/num_of_samples\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(1 + first_term - second_term)\n",
    "\n",
    "  # print the cost\n",
    "    print(1 + first_term - second_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 0.08\n",
    "num_of_samples = 200\n",
    "dimension = 2\n",
    "hidden_layer = 5\n",
    "deviation = -1\n",
    "\n",
    "# initialize the neural network and quantum circuit parameters\n",
    "W = deviation*np.random.rand(3,3)\n",
    "np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                forward_sum += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the forward sum\n",
    "            forward_sum = forward_sum/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum = 0\n",
    "            result = measure_rho(shifted)\n",
    "            result = list(result)\n",
    "            result[0] = list(map(int, result[0]))\n",
    "            result[1] = list(map(int, result[1]))\n",
    "            for sample in range(num_of_samples):\n",
    "                backward_sum += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum = backward_sum/num_of_samples\n",
    "      #print(backward_sum)\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] = - 0.5 * (forward_sum - backward_sum)\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init -= learning_rate*gradients\n",
    "  #print(gradients)\n",
    "\n",
    "  # evaluate the gradient with respect to the NN parameters\n",
    "\n",
    "\n",
    "  # evaluate the gradient with respect to the eigenvalues\n",
    "\n",
    "  # 1 , 1\n",
    "    E = np.zeros_like(W)\n",
    "    E[1][1] = 1\n",
    "    dW1 = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW1 -= E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "\n",
    "    # normalize it\n",
    "    dW1 = dW1/num_of_samples\n",
    "    dW1 += (np.exp(W[1][1])/N)\n",
    "\n",
    "    # 1 , -1\n",
    "    E = np.zeros_like(W)\n",
    "    E[1][-1] = 1\n",
    "    dW2 = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW2 -= E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "\n",
    "    # normalize it\n",
    "    dW2 = dW2/num_of_samples\n",
    "    dW2 += (np.exp(W[1][-1])/N)\n",
    "\n",
    "    # -1 , 1\n",
    "    E = np.zeros_like(W)\n",
    "    E[-1][1] = 1\n",
    "    dW3 = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW3 -= E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "\n",
    "    # normalize it\n",
    "    dW3 = dW3/num_of_samples\n",
    "    dW3 += (np.exp(W[-1][1])/N)\n",
    "\n",
    "    # -1 , -1\n",
    "    E = np.zeros_like(W)\n",
    "    E[-1][-1] = 1\n",
    "    dW4 = 0\n",
    "    result = measure_rho(prev_param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        dW4 -= E[result[0][sample]][result[1][sample]]\n",
    "\n",
    "\n",
    "    # normalize it\n",
    "    dW4 = dW4/num_of_samples\n",
    "    dW4 += (np.exp(W[-1][-1])/N)\n",
    "\n",
    "\n",
    "    # update the eigenvalues\n",
    "    W[1][1] -= learning_rate*dW1\n",
    "    W[1][-1] -= learning_rate*dW2\n",
    "    W[-1][1] -= learning_rate*dW3\n",
    "    W[-1][-1] -= learning_rate*dW4\n",
    "\n",
    "    # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    result = list(result)\n",
    "    result[0] = list(map(int, result[0]))\n",
    "    result[1] = list(map(int, result[1]))\n",
    "    for sample in range(num_of_samples):\n",
    "        first_term += W[result[0][sample]][result[1][sample]]\n",
    "\n",
    "    # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "    # # Second term evaluation\n",
    "    second_term = np.exp(W[1][1]) + np.exp(W[1][-1]) + np.exp(W[-1][1]) + np.exp(W[-1][-1])\n",
    "\n",
    "    # normalize the second term sum\n",
    "    second_term = second_term/N\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(first_term - second_term + 1)\n",
    "\n",
    "    # print the cost\n",
    "    print(first_term - second_term + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3412a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_function(nn.Module):\n",
    "    def __init__(self,dimension,hidden_layer):\n",
    "        super(neural_function, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.lin1 = nn.Linear(self.dimension, self.hidden_layer)\n",
    "        self.lin_end = nn.Linear(self.hidden_layer, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # y = Fn.leaky_relu(self.lin1(input))\n",
    "        y = torch.sigmoid(self.lin1(input.float()))\n",
    "        y = self.lin_end(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46470cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimization using Gradient Descent\n",
    "\n",
    "# parameters of the optimization\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 0.05\n",
    "num_of_samples = 100\n",
    "dimension = 2\n",
    "hidden_layer = 10\n",
    "\n",
    "# initialize the neural network and quantum circuit parameters\n",
    "neural_fn = neural_function(dimension, hidden_layer)\n",
    "param_init = np.random.random(qml.RandomLayers.shape(n_layers=num_layers, n_rotations=3))\n",
    "\n",
    "# intialize the cost function store\n",
    "cost_func_store = []\n",
    "\n",
    "\n",
    "# start the training\n",
    "for epoch in range(1, num_of_epochs):\n",
    "\n",
    "  # evaluate the gradient with respect to the quantum circuit parameters\n",
    "    gradients = np.zeros_like((param_init))\n",
    "    for i in range(len(gradients)):\n",
    "        for j in range(len(gradients[0])):\n",
    "\n",
    "      # copy the parameters\n",
    "            shifted = param_init.copy()\n",
    "\n",
    "      # right shift the parameters\n",
    "            shifted[i, j] += np.pi/2\n",
    "\n",
    "      # forward evaluation\n",
    "            forward_sum = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[0][sample], result[1][sample]])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                forward_sum += nn_result[0].detach().numpy()\n",
    "\n",
    "      # normalize the forward sum\n",
    "            forward_sum = forward_sum/num_of_samples\n",
    "\n",
    "      # left shift the parameters\n",
    "            shifted[i, j] -= np.pi\n",
    "\n",
    "      # backward evaluation\n",
    "            backward_sum = 0\n",
    "            result = measure_rho(shifted)\n",
    "            for sample in range(num_of_samples):\n",
    "                sample_result_array = np.array([result[0][sample], result[1][sample]])\n",
    "                nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "                backward_sum += nn_result[0].detach().numpy()\n",
    "\n",
    "      # normalize the backward sum\n",
    "            backward_sum = backward_sum/num_of_samples\n",
    "      #print(backward_sum)\n",
    "\n",
    "      # parameter-shift rule\n",
    "            gradients[i, j] = - 0.5 * (forward_sum - backward_sum)\n",
    "\n",
    "  # first copy the quantum circuit parameters before updating it\n",
    "    prev_param_init = param_init.copy()\n",
    "\n",
    "  # update the quantum circuit parameters\n",
    "    param_init -= learning_rate*gradients\n",
    "  #print(gradients)\n",
    "\n",
    "  # evaluate the gradient with respect to the NN parameters\n",
    "\n",
    "\n",
    "  # evaluate the first term\n",
    "    grad_w1 = torch.zeros_like(neural_fn.lin1.weight)\n",
    "    grad_b1 = torch.zeros_like(neural_fn.lin1.bias)\n",
    "    grad_w2 = torch.zeros_like(neural_fn.lin_end.weight)\n",
    "    grad_b2 = torch.zeros_like(neural_fn.lin_end.bias)\n",
    "\n",
    "    result = measure_rho(prev_param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[0][sample], result[1][sample]])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        nn_result.backward()\n",
    "        grad_w1 += neural_fn.lin1.weight.grad*(1/num_of_samples)\n",
    "        grad_b1 += neural_fn.lin1.bias.grad*(1/num_of_samples)\n",
    "        grad_w2 += neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2 += neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        neural_fn.lin1.weight.grad.zero_()\n",
    "        neural_fn.lin1.bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "  # evaluate the second term\n",
    "    grad_w1_2 = torch.zeros_like(neural_fn.lin1.weight.grad)\n",
    "    grad_b1_2 = torch.zeros_like(neural_fn.lin1.bias.grad)\n",
    "    grad_w2_2 = torch.zeros_like(neural_fn.lin_end.weight.grad)\n",
    "    grad_b2_2 = torch.zeros_like(neural_fn.lin_end.bias.grad)\n",
    "\n",
    "    for sample in range(num_of_samples):\n",
    "        result = np.random.choice([-1, 1], size=2)\n",
    "        nn_result = neural_fn(torch.from_numpy(result.flatten()))\n",
    "        nn_result.backward()\n",
    "        grad_w1_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin1.weight.grad*(1/num_of_samples)\n",
    "        grad_b1_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin1.bias.grad*(1/num_of_samples)\n",
    "        grad_w2_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin_end.weight.grad*(1/num_of_samples)\n",
    "        grad_b2_2 += (np.exp(nn_result[0].detach().numpy()))*neural_fn.lin_end.bias.grad*(1/num_of_samples)\n",
    "        neural_fn.lin1.weight.grad.zero_()\n",
    "        neural_fn.lin1.bias.grad.zero_()\n",
    "        neural_fn.lin_end.weight.grad.zero_()\n",
    "        neural_fn.lin_end.bias.grad.zero_()\n",
    "\n",
    "  # evaluate the difference, i.e., the gradient\n",
    "\n",
    "    nn_grad_W1 = grad_w1_2 - grad_w1\n",
    "    nn_grad_b1 = grad_b1_2 - grad_b1\n",
    "    nn_grad_W2 = grad_w2_2 - grad_w2\n",
    "    nn_grad_b2 = grad_b2_2 - grad_b2\n",
    "\n",
    "  # update the NN weights and normalize them\n",
    "    with torch.no_grad():\n",
    "        neural_fn.lin1.weight -= learning_rate*nn_grad_W1\n",
    "        neural_fn.lin1.bias -= learning_rate*nn_grad_b1\n",
    "        neural_fn.lin_end.weight -= learning_rate*nn_grad_W2\n",
    "        neural_fn.lin_end.bias -= learning_rate*nn_grad_b2\n",
    "\n",
    "  # evaluate the cost function at these parameters\n",
    "    first_term = 0\n",
    "    result = measure_rho(param_init)\n",
    "    for sample in range(num_of_samples):\n",
    "        sample_result_array = np.array([result[0][sample], result[1][sample]])\n",
    "        nn_result = neural_fn(torch.from_numpy(sample_result_array))\n",
    "        first_term += nn_result[0].detach().numpy()\n",
    "\n",
    "  # normalize the cost sum\n",
    "    first_term = first_term/num_of_samples\n",
    "\n",
    "  # # Second term evaluation\n",
    "    second_term = 0\n",
    "    for sample in range(num_of_samples):\n",
    "        result = np.random.choice([-1, 1], size=2)\n",
    "        nn_result = neural_fn(torch.from_numpy(result.flatten()))\n",
    "        second_term += np.exp(nn_result[0].detach().numpy())\n",
    "\n",
    "  # normalize the second term sum\n",
    "    second_term = second_term/num_of_samples\n",
    "\n",
    "    # add the cost function to the store\n",
    "    cost_func_store.append(first_term - second_term + 1)\n",
    "\n",
    "  # print the cost\n",
    "    print(first_term - second_term + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
